PROFESSIONAL SUMMARY: Around 11years of Data Discovery & Analytics, Data Modeling, ETL Design, Development, Reporting, Testing, Implementation and Production supportin the field of large scale Data Warehousing and Business Intelligence projects. Very strong business intelligence experience specializing inRDBMS, ETL Concepts, Design & Architecture, ETL and Reporting Development & Testing Strong in Reporting tools like SAP Business Objects, Qlikview, Tableau, Actuate & SSRS Reports Wide versatile domain experience in various industries like Health Care, Insurance, Financial services. Strong in data profiling for data requirements supporting business and post production data quality and performance checks. Very strong on Informatica DQ to cleanse the data with IDQ specific transformations like labeler, standardizer, address validator, consolidation transformations for data validation logics. Exported IDQ mappings as mapplets and integrated with Power center mappings. Worked on Informatica BDM development, testing, Hadoop cluster configurations and migrations. Developed and executed mappings through informatica developer on Hadoop clustering with Hive query connection. Monitored the Hadoop cluster jobs, scripts, hive queries, map reduce programs through informatica Monitoring tool Worked on Hadoop file system and data loads & analysis through Big Data tools like Hive, Sqoop, PIG etc. An expert inETL Tools - Informatica & Talendwhich includes components likePower Center, Designer, Workflow Manager, Workflow Monitor, Repository Manager, Repository Server Administration Console, Power Exchange, IDQ - Informatica Data Quality, Informatica Visio Data Stencil, Talend Integration data studio. Have had extensive experience in working with various Power Center Transformations using Designer Tool. Have handled various Data Sources likeNetezza, Teradata, DB2, Oracle, SQL Server, Microsoft Access, MS Excel including Flat Files, XML Files and Web services. Worked extensively on Salesforce CRM integration with Informatica mappings. Very versed with Netezza and Teradata bulk load utilities like nzload, Fastload, Mload, Tpump, BTEQ processes for import/export of huge data files with databases. Good experience inUnix Shell ScriptingandETL Process Automation - implementation of Performance Tuning techniques at both Database & Informatica level. Proficiency inOracle PL/SQL and SQL Analytical functions. Expert in creation of stored procedures and custom SQL to define ETL processes. Worked on real time data sourcing like Informatica with cloud using Informatica Intelligent Cloud Services (IICS), Salesforce in Informatica mappings. Worked on both Software Development Life Cycle (SDLC) and agile methodologies (Scrum) on data integration projects. Have in-depth experience in ITIL framework and Service Management and Scheduling processes. Extensively worked on related tools like HP Service Manager, Autosys & Tivoli. Strong Logical and Analytical Reasoning Skills, Excellent Communication with good Listening, Presentation and Interpersonal Skills. Highly motivated to take independent responsibility, collaboration, strong in team building and mentoring skills showing excellent Team leadership capability. TECHNICAL SKILLS: BI ToolsInformatica:: Power center 10.1/9.6/9.1, Informatica Power Exchange, Informatica Data Quality 10.1/9.6, Informatica Analyst, Metadata manager (MDM), Informatica BDM, Informatica MDM, Talend 7.01 Business Objects, Qlikview, Tableau, SSRS, Actuate and Jasper Data Modeling: Star schema & Snow flake schema modelling, Fact-Dimension modelling, ER Modelling, Pgmodeler Databases/ CRM: Netezza, Teradata, Oracle, Salesforce, SQL Server, Sybase, IBM DB2, postgresQL 9.6.1, SAP HANA Languages: SQL, PL/SQL, Unix Shell Scripting, Perl Scripting, Python, Java, J2EE, Hive, Sqoop, PIG IDE / Tools: SQL Developer, WinSQL, Aginity, Toad, Rapid SQL, Erwin, Power Designer, Power BI, Humming bird, CoreFTP, winscp, Putty, VSS, PVCS, TFS, Eclipse, RSA/RAD, Ms Office suite including Ms Access, Visio, Excel, Salesforce apex data loader, Pgadmin III, Postgres compare, Big bucket Job Scheduling tools: Autosys, Workload Command Center, Tivoli (TWS), Tidal Automation Service Management: HP Service Manager, Service now, JIRA Frameworks / APIs: Struts 2.0 Hibernate 3.2, Spring, JFreechart and JXL. Operating Systems: Windows, Unix PROFESSIONAL EXPERIENCE: Confidential  Lead ETL developer / Senior Data Analyst Responsibilities: Understanding source data model and business specifications from clients and analyse technical requirements Data profiling on data sources using Informatica Analyst tool Worked on Architecture data flow, ETL data modelling/design. Developed multiple data elements through ETL development using Talend & Informatica BDM. Worked extensively on PostgreSQL using pgadmin tool. Developed and executed mappings through informatica BDM and Talend ETL tools. Code and Test cases review on the code logics and test cases developed by team. Well versed with Bit buckest version controller. Deployment planning and release management Performance optimization on mappings, sessions, SQL queries Team Oriented, client negotiation and effective communication between critical stakeholders - Business partners and IS clients. Environment: UNIX, DB Vizualizer, Dbeaver, Erwin, Pgmodeler, Postgres compare, Pgadmin III, PostgreSQL 9.6.1, Oracle, SQL Developer, Informatica Power center 10.1, Talend 7.0.1, Informatica BDM 10.1, Informatica Data quality 10.1, Informatica Analyst, Bit bucket Confidential Lead ETL Developer/ Sr Data Analyst Responsibilities: Understanding client business specifications through meetings with data architect and analyse technical requirements Data profiling on data sources using Informatica Analyst tool Worked on Architecture data flow, ETL data modelling/design. Have taken care of data migration project designing complex ETLs involving huge flat files, XML sources, transformation logics like - cummulative balances, monthly averages, running multiple batches, concurrent sessions using Talend & Informatica Worked on PostgreSQL, SAP HANA, Oracle and sql server data base for adhoc queries and analysis. Developed and executed mappings through informatica and Talend ETL tools. ETL code integration with SAP system as source & target Execution of ETL jobs through SAP BW scheduler. Worked widely in development and execution of jobs in both UNICODE and NON-UNICODE modes in Informatica. Code and Test cases review on the code logics and test cases developed by team. Involved in Informatica project migrations, code deployments, access privileges. Deployment planning and release management Performance optimization on mappings, sessions, SQL queries Tidal Job scheduling in various testing environments and in production environment during project implementation. Team Oriented, client negotiation and effective communication between critical stakeholders - Business partners and IS clients. Environment: UNIX, Winscp, Python, Putty, winSQL, Netezza, Teradata, PostgreSQL, Tableau, Oracle, SQL Developer, Informatica Power center 10.1, Talend, Informatica Data quality 10.1, Informatica Intelligent Cloud Services (IICS), Informatica Analyst, Salesforce, Tidal, JIRA, Power Designer, Informatica BDM, Hive, Sqoop Confidential, Columbia, Maryland Sr ETL Developer/ Data Analyst Responsibilities: Understanding client business specifications through meetings with data architect and analyse technical requirements Data profiling on data sources using Informatica Analyst tool Worked on Architecture data flow, ETL data modelling/design. Have taken care of data migration project designing complex ETLs involving huge flat files, XML sources, transformation logics like - cummulative balances, monthly averages, running multiple batches, concurrent sessions using Talend & Informatica Well versed with loading the data into OFSAA (Oracle Financial Services Analytical Applications) and exporting analytical data reports. Developed IDQ mappings to cleanse the data and validations mainly email and address validations through address validator, labeller, standardizer transformation with the help of address doctor plug in. Exported IDQ mappings as mapplets and integrated with power center mappings Implemented real time data sources through Informatica Power exchange CDC logic on Oracle and Mainframe (legacy) systems through datamaps. Worked on PostgreSQL database management system to store secure data like PII and SII data elements. Developed and executed mappings through informatica developer on Hadoop cluster with Hive query connection. Monitored the Hadoop cluster jobs, scripts, hive queries, map reduce programs through informatica Monitoring tool Worked on Informatica BDM development, testing in reading and writing the data in HDFS files Developed python scripts to load huge scottrade transactional data file through netezza nzload utitlity (bulk loader) and Teradata bulk loads (Fastload, Mload, BTEQ) Reporting data validation through SQL and Hive tables and queries Developed code logics for converting scottrade transactional to analytical data with various informatica transformations. Created optimized framework and saved USD 5K effort to populate running balances data for weekends based on week day data for the sake of monthly reporting analytics. Worked extensively with real time data sourcing like Informatica with cloud, Salesforce in Informatica mappings. Worked on change data capture (CDC) for oracle data sources in Informatica Intelligent Cloud Services (IICS) Code and Test cases review on the code logics and test cases developed by team. Involved in Informatica project migrations, code deployments, access privileges. Deployment planning and release management Performance optimization on mappings, sessions, SQL queries Tidal Job scheduling in various testing environments and in production environment during project implementation. Team Oriented, client negotiation and effective communication between critical stakeholders - Business partners and IS clients. Environment: UNIX, Winscp, Python, Putty, winSQL, Netezza, Teradata, PostgreSQL, Tableau, Oracle, SQL Developer, Informatica Power center 10.1, Talend, Informatica Data quality 10.1, Informatica Intelligent Cloud Services (IICS), Informatica Analyst, Salesforce, Tidal, JIRA, Power Designer, Informatica BDM, Hive, Sqoop Confidential Sr ETL & IDQ Developer / Data analyst Responsibilities: Understanding client business specifications through meetings with data architect and analyse technical requirements Worked on Architecture data flow, documentation of ETL design documents. Data profiling for data requirements supporting business and post production data quality and performance checks Developed code logics in creating mappings to pull the data from Salesforce object and load into Netezza database and flat files. Developed ETL code logics using Informatica and Talend ETL tools. Worked on informatica IDQ mapplets with different transformations like standardizer, labeller, address validator etc. Presented IDQ sessions within my team including IDQ integration with Informatica power center and its features. Worked extensively with Salesforce objects as sources, targets and lookups in Informatica mappings. Worked extensively with XML files as sources in Informatica mappings. Code and Test cases review on the code logics and test cases developed by team. Involved in Informatica, Talend and Database objects migrations Deployment planning and release management Performance optimization on mappings, sessions, SQL queries Tidal Job scheduling in various testing environments and in production environment during project implementation. Worked Independently, client negotiation and effective communication between critical stakeholders - Business partners and IS clients. Environment: UNIX, Winscp, Putty, Kafka, winSQL, Netezza, PostgreSQL, Oracle, SQL Developer, XML files, Informatica MDM, Informatica Power center 10.1, Salesforce, Informatica IDQ, Tidal, Qlikview, Tableau Confidential, Newark, New Jersey Senior ETL Developer / Data Analyst Responsibilities: Understanding client business specifications and analyse technical requirements Data profiling on data sources using Informatica Analyst tool Worked on Architecture data flow, documentation of Technical Requirements (TRD) and ETL design documents. Developed code logics for complex requirements with the help of complex SQL queries, Oracle analytical functions and with various informatica transformations. Developed ETL code logics using Informatica and Talend ETL tools. Developed stored procedure, SQLs using PostgreSQL database management system. Worked extensively with real time data sourcing like Informatica with cloud, Salesforce in Informatica mappings. Implemented complex business requirements like creating multiple entries, wildcarding / grouping the records as per their benefit information through informatica transformations. Implemented CBR (Checks & Balances records) model to validate and document the record counts, balances across the data flow till outbound extracts. Code and Test cases review on the code logics and test cases developed by team. Proposed the best practices and naming conventions for the ETL and ensured that the team has implemented them and peer review was done before finalizing the code. Involved in Informatica and Database objects migrations Deployment planning and release management Performance optimization on mappings, sessions, SQL queries and stored procedures. Tivoli Job scheduling and Service management during migrations to different environments. Team Oriented, client negotiation and effective communication between critical stakeholders - Business partners and IS clients. Environment: UNIX, CoreFTP, Putty, SQL Server, Oracle, PostgreSQL, SQL Developer, Informatica MDM, Informatica Power center and Power Exchange 9.1, IDQ Informatica Developer/Informatica, Toad, Tivoli, HPSM, Service Now Confidential  Sr ETL & IDQ Developer Responsibilities: Understanding client specifications, functional requirements Data profiling on data sources using Informatica Analyst tool Worked on Architecture data flow, documentation of Technical Requirements (TRD) and ETL design documents using Erwin. Designed complex mappings, peer review on unit test cases and system test cases prepared by team to validate the data load. Developed IDQ mappings and converted them as power center mappings to make development faster with parallel unit testing. Data cleansing and quality checks using IDQ transformations & mapplets. Data analysis and writing SQLs on SAP HANA database Implemented and well versed with advanced transformations like transaction control, java, stored procedure, web services and XML transformations. Worked with various data file formats like EDI 837 and HL7 transaction files coming from public organizations. Bulk data load import & export using Sqoop and reporting using Hive queries etc Worked with SAP sources and targets integration with informatica and scheduling the jobs through SAP BW scheduler. Worked on Medicaid, medicare transactions Has experience on Python scripting with spark. Good knowledge on Rstudio and Informatica metadata manager tools. Created many reusable artifacts - Table driven referential integrity using stored procedure transformation, single point MD5 keys, macro embedded excel tool for creation of Tivoli job scripts. Worked on Version controlled Informatica with features like check-in, check outs, applying labels for code deployments and migrations across the environments. Performance tuning of the mappings, sessions, complex SQL queries and stored procedures. Designed the workflow architecture successfully to optimize the end to end run time and scheduled the workflows using Tivoli job scheduling tool. Team Oriented, client negotiation and effective communication between critical stakeholders - Business partners and IS clients Environment: UNIX, CoreFTP, Putty, Teradata, Oracle, DB2, SAP HANA, SQL Developer, kafka, Talend, Informatica Power center and Power Exchange 9.1, IDQ Informatica Developer/Informatica Analyst, Informatica MDM, Toad, Tivoli, HPSM, Service Now, Hive, Sqoop, Hadoop HDFS system, Informatica BDM, Qlikview Confidential Application Engineer/Sr. IT Support Specialist / Tech Lead Responsibilities: Designed data mapping documents with build rules etc Developed mappings/sessions/workflows for the enhancement logics involving forwarding logics, CDC logic using MD5 keys. Developed oracle PL/SQL stored procedures and functions to implement looping logics. Developed Informatica workflows with event based and file based scheduling, decision control, command and email tasks Worked in conversion of existing Actuate reports to developing SSRS reports for enhancements. Well versed with code deployments and production release procedures, ITIL Service management using HPSM, Service Now tools. Created jobs, boxes and setup schedules in Autosys scheduling tool and version control tools like PVCS, TFS, VSS. Troubleshooting, in-depth analysis for system issues and implementing bug fixes with optimized solutions. Worked as Subject Matter Expert (SME) for data warehousing and Reporting Systems. Support Analysis & Design process from a technical and feasibility point of view. Business clients query resolution on consistent and data driven business intelligence reports. Performance improvement ideas to enhance nightly batch cycle SLA and reduce online based applications downtime. Environment: UNIX, Oracle, DB2, SQL Developer, Informatica Power center, Exceed, Tivoli(TWS), HPSM, Service Now, SSRS, Autosys Confidential  Senior System Analyst / ETL Developer Responsibilities: Troubleshooting, in-depth analysis of existing system spanning multiple technologies - Java, UNIX, Informatica, Actuate and bug fixes in existing applications. Working collaboratively with multiple teams on upstream data feed and monitor nightly batch process on all the data loads. Identify bottlenecks & provide optimal solutions for performance improvements and optimization. Implementing performance optimization/tuning on Informatica code and database level Developed ad-hoc business objects reports for business clients Implemented enhancements in actuate and SSRS reports Proving estimations and reusable ideas for improving ROI significantly. Providing warranty support mainly on-call query resolution to clients with my technical knowledge and required data analysis. Extensively used service management framework mostly Change, Release and Knowledge management. Developing reusable tools, played KM anchor role and tracking metrics w.r.t Goals Environment: UNIX, Oracle 11g, Sybase, DB2, Rapid SQL, Informatica Power center and Power Exchange 9.1, Actuate 11, Business Objects 4.0, MQ, Serena PVCS, Autosys, Struts 2.0 Confidential  SQL Developer Responsibilities: Client Co-ordination, Gathering requirements & User Specifications Created high level and low level design documents stating all functionalities in the application Developed web based application using Struts 2 and Hibernate frameworks Implemented complex module with writing complex SQL queries and DB stored procedures & functions, mainly used table data types (Object and Type) as input and output parameters.Performed data migration activities from MS Access to Oracle database. Prepared and reviewed Unit & QA test plans. Post implementation support, Change & Release Management Environment: Java, J2EE, Struts 2.0, Spring (IOC), Hibernate 3.5, JSP, JavaScript, HTML, Oracle 10g, SQL, TOAD, RAD, Windows XP, Ms Office Tools. Confidential Developer Responsibilities: Use case specifications, requirements gathering and analysis Created High level and detailed design documents of the solution Developed web based application using Struts 2 and Hibernate frameworks Worked on modules, which presents the information in both tabular and graphical as per client’s convenience. Demonstrated the functionalities of applications with the help of demos to clients Implementation / Technical support and user query resolution Environment: Java, J2EE, Struts 2.0, Hibernate 3.5, JSP, Javascript, SQL, Oracle 10g, Windows XP, Microsoft office tools (Excel, Access etc.), TOAD, eclipse 