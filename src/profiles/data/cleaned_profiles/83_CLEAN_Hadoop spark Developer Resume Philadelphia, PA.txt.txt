SUMMARY Around 7 years of experience in the field of Information Technology which includes a major concentration on Big Data and Hadoop Ecosystems, various Relational Databases and NoSQL Databases, Java Programming language and J2EE technologies. Extensive experience of development using Hadoop ecosystem components like Hadoop MVR1/MVR2, Spark, Hive, HDFS, Kafka, Impala, HBase, MapReduce, Pig, Sqoop, YARN, Zoo Keeper and Oozie. Good Understanding of Hadoop Gen1/Gen2 architecture and Hands - on experience with Hadoop components such as Job Tracker, Task Tracker, Name Node, Data Node, Map Reduce concepts and YARN architecture. Experience in Cloudera, Hortonworks Hadoop distribution and maintaining optimized AWS infrastructure (EMR, EC2, S3, EBS) Hands on experience in different phases of big data applications like data ingestion, data analytics and data visualization. Experience in using distributed computing architectures such as AWS products (e.g. EC2, EMR, Elastic search), Hadoop, Spark (Spark with Scala) and effective use of map-reduce, SQL and HBase to solve Big Data requirements. Hands on experience in big data ingestion tools like Flume and Sqoop. Experience in importing and exporting data using Sqoop to HDFS, Hive from Relational Database Systems and vice-versa. Experience in transferring Streaming data, data from different data sources into HDFS, No SQL databases using Apache Flume. Experience in using different file formats like Sequence, AVRO, ORC files, Parquet and CSV using different compression Techniques such as Codec and Snappy. Expert in implementing advanced procedures like text analytics and processing using the in-memory computing capabilities with Apache Spark written in Scala. Involved in converting Hive/SQL queries into Spark transformations using Spark RDDs and Spark SQL. Experience in using accumulators, Broadcast variables, various levels of caching and optimization techniques in spark. Experience in tuning the performances by using Partitioning and Bucketing in Hive. Hands on experience in writing Ad-hoc Queries for moving data from HDFS to HIVE and analyzing the data using HIVE QL. Developed Pig Latin scripts for data cleansing and Transformation. Familiar with Apache Kafka, spark with YARN, Local and Standalone modes. capable of processing large sets of structured, semi-structured, un-structured data and supporting systems application architecture. Good Knowledge on Hadoop Cluster architecture and monitoring the cluster. Strong Knowledge on Spark concepts like RDD Operations, Caching and Persistence. Experienced in creating various tables in Hive which include Managed Tables and External tables and loading data into Hive from HDFS/local file system. Extending HIVE and PIG core functionality by using custom User Defined Function’s (UDF). Knowledge in job/workflow scheduling and monitoring tools like Oozie and Zookeeper. Good knowledge of No-SQL databases such as HBase and MongoDB. Good knowledge on Apache Hadoop Cluster planning which includes choosing the Hardware and operating systems to host an Apache Hadoop cluster. Strong programming experience using Scala, Java and SQL. Experience in database design using PL/SQL to write Stored Procedures, Functions, Triggers and strong experience in writing complex queries for Oracle. Experience in developing web pages quickly and effectively using HTML, CSS and JavaScript. Strong knowledge in using Object Oriented Programming concepts of JavaScript and working knowledge of DOM models. Built complex web applications using Model-View-Controller (MVC) architecture. Expertise in debugging and troubleshooting existing code. Experience with SVN, TFS and GIT for code management and version control in collaborated projects. Experience in working with agile methodologies such as Scrum, Waterfall Model and Test-Driven Development. Developed web application using ASP.NET, CSS, C# and SQL Server with Visual Studio 2012 as the IDE. In-depth knowledge working with various databases such as Oracle 11g, MySQL, MongoDB Expertise in object modeling and Object-Oriented design methodologies (UML). Experience with UML diagrams like Class, Object, Use Case, State, and Activity diagrams. TECHNICAL SKILLS Hadoop Core Services: HDFS, Map Reduce, YARN, TEZ Hadoop Distributions: Hortonworks, Cloudera CDH3/4/5 Big Data Ingestion: Apache Sqoop, Apache Kafka, Apache Flume, Apache Spark, Storm Big Data Analysis: Hive, Pig, Spark, Storm, Impala Cluster Management Tools: Ambari, Zookeeper, Oozie, Hue, Cloudera Manager Frameworks: Hadoop, Spark Amazon Technologies: EC2, S3, EMR, Dynamo DB, IAM, SNS, SQS, Route53 Web Technologies: HTML, CSS, Bootstrap, JavaScript, jQuery IDE Tools/Editors: IntelliJ, Notepad++, Sublime Text 3, visual studio 2012, Eclipse Databases: Oracle 10g/11g, MYSQL, Netezza, MongoDB Programming Languages: Scala, Java Script, Core Java, SQL, PL/SQL, C#, C, Linux Shell Scripting Version Controls: GIT, SVN, TFS Debugging Tools: Firebug, Firebug Lite, Chrome or Safari web inspectors, Grunt, Karma. Methodologies: Agile/Scrum, Waterfall. PROFESSIONAL EXPERIENCE Confidential, Philadelphia, PA Hadoop/spark Developer Responsibilities: Working on Big Data infrastructure for batch processing as well as real-time processing. Responsible for building scalable distributed data solutions using Hadoop. Responsible for ingesting large volumes of user behavioral data and customer profile data to Analytics Data store. Converted existing MapReduce jobs into Spark transformations and actions using Spark RDDs, Data frames and Spark SQL APIs. Wrote new spark jobs in Scala to analyze the data of the customers and sales history. Involved in requirement analysis, design, coding and implementation phases of the project. Experience in designing and developing applications in Spark using Scala to compare the performance of Spark with Hive and SQL/Oracle. Used Spark API over Hadoop YARN to perform analytics on data in Hive. Experience in both SQL Context and Spark Session Developed Scala based Spark applications for performing data cleansing, event enrichment, data aggregation, de-normalization and data preparation needed for machine learning and reporting teams to consume. Worked on troubleshooting spark application to make them more error tolerant. Worked on fine-tuning spark applications to improve the overall processing time for the pipelines. Experienced in handling large datasets using Spark in Memory capabilities, using broadcasts variables in Spark, effective and efficient Joins, transformations and other capabilities. Loaded the data into Hive tables from spark and used parquet columnar format. Worked extensively with Sqoop for importing data from Oracle. Experienced in working with spark eco system using Spark SQL and Scala queries on different formats like Text file, CSV file. Created Sqoop jobs to handle incremental loads from RDBMS into HDFS and applied Spark transformations. Developed Oozie workflows to automate and product ionize the data pipelines. Developed Sqoop import Scripts for importing reference data from Oracle Experience working for EMR cluster in AWS cloud and working with S3. Involved in creating Hive tables, loading and analyzing data using hive scripts. Implemented Static partitions, Dynamic partitions and Buckets in HIVE. Used Reporting tools like Tableau to connect with Impala for generating daily reports of data. Collaborated with the infrastructure, network, database, application and BA teams to ensure data quality and availability. Designed, documented operational problems by following standards and procedures using JIRA. Used Kafka to get data from many streaming sources into HDFS. Environment: Hadoop 2.x, Spark, Scala, Hive, Sqoop, Oozie, Kafka, Amazon EMR, Zookeeper, Impala, YARN, JIRA, Amazon AWS, Shell Scripting, SBT, GITHUB. Confidential, Durham, NC Hadoop Developer Responsibilities: Developed various data loading strategies and performed various transformations for analyzing the datasets by using Cloudera Distribution for Hadoop ecosystem. Imported/exported data from/to RDBMS-HDFS using Data Ingestion tools like Sqoop Analyzed large data sets by running Hive queries and wrote UDF's. Worked in Loading and transforming large sets of structured, semi structured and unstructured data. Experienced in handling Avro and Json data in Hive using Hive SerDe's. Implemented Partitions, bucketing concepts in Hive and designed both Managed and External tables in Hive for optimized performance Involved in collecting, aggregating and moving data from servers to HDFS using Flume. Collected data from various Flume agents that are imported on various servers using multi hop Flow. Knowledge on various flume sources, channels and sink by which data is ingested into HDFS Responsible for performing various transformations like sort, join, aggregations, filter to retrieve various datasets using Spark. Experience in extracting appropriate features from datasets in-order to handle bad, null, partial records using spark SQL. Worked on storing the data frame into hive as table using Spark SQL. Experienced in ingesting data into HDFS from various Relational databases like Teradata using Sqoop and exported data back to Teradata for data storage. Hands on experience in developing SPARK applications using Spark tools like Spark core, Spark Streaming and Spark SQL. Experience in developing various spark application using Spark-shell(Scala). Involved in creating Hive Tables, loading with data and writing Hive queries which will invoke and run MapReduce jobs in the backend. Designed and implemented Incremental Imports into Hive tables and writing Hive queries which on TEZ. Migrated ETL jobs to Pig scripts do Transformations, even joins and some pre-aggregations before storing the data onto HDFS. Involved in writing optimized Pig Script along with developing and testing Pig Latin Scripts Implemented the workflows using Apache Oozie framework to automate tasks Worked on different file formats like Sequence files, XML files and Map files using MapReduce Programs. Worked with Avro, Parquet and ORC File data formats. Written HBase bulk load jobs to load processed data to HBase tables by converting to HFiles. Exported data to MongoDB(NoSQL) database from HDFS using Sqoop and performed various commands on to obtain various datasets as required. Created and imported various collections, documents into MongoDB and performed various actions like query, project, aggregation, sort, limit. Involved in Unit testing and delivered Unit test plans and results documents using Junit. Created Hive UDFs and UDAFs using shell scripts and Java code based on the given requirement. Automated all the jobs to pull the data and load into Hive tables, using Oozie workflows Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior. Created and maintained Technical documentation for launching Hadoop Clusters and for executing Pig Scripts. Environment: Hadoop, HDFS, Map Reduce, spark, Sqoop, Oozie, Pig, Hive, Flume, TEZ, LINUX, Java, Eclipse, Shell scripting, MongoDB. Confidential, Schaumburg, Illinois Hadoop Developer Responsibilities: Worked with Big Data team responsible for buildingHadoopstack and different big data analytic tools, migration from RDBMS to Hadoop using Sqoop. Installed and configured Hadoop MapReduce, HDFS, Developed multiple MapReduce jobs in java for data cleaning and preprocessing. Developed custom MapReduce programs and custom User Defined Functions (UDF’s) in Hive to transform the large volumes of data with respect to business requirement. Wrote MapReduce jobs using Java API and Pig Latin. Extracted the data from the flat files and other RDBMS databases into staging area and ingested to Hadoop HDFS/Hive. Involved in creating Hive tables, loading with data and writing hive queries which will run internally in MapReduce way. Involved in migrating tables from RDBMS into Hive tables using SQOOP and later generate visualizations using Tableau. Developed numerable Pig batch programs for both implementation, and optimization needs. Used HBase in accordance with Hive/Pig as per the requirement. Created different Pig scripts and converted them as a shell command to provide aliases for common operation for project business flow. Load the data into HDFS from different Data sources like Oracle, DB2 using Sqoop and load into Hive tables. Integrated the hive warehouse with HBase for information sharing among teams. Developed complex Hive UDFs to work with sequence files. Designed and developed Pig Latin scripts and Pig command line transformations for data joins and custom processing of MapReduce outputs. Created dashboards in Tableau to create meaningful metrics for decision making. Performed rule checks on multiple file formats like Avro, Parquet, JSON, CSV and compressed file formats. Monitored System health and logs and respond accordingly to any warning or failure conditions. Implemented Counters for diagnosing problem in queries and for quality control and application-level statistics. End-to- end performance tuning of Hadoop clusters and Hadoop Map/Reduce routines against very large data sets. Optimized Map/Reduce Jobs to use HDFS efficiently by using various compression mechanisms Involved in defining job flows using Oozie for scheduling jobs to manage apache Hadoop jobs. Involved in Agile methodologies, daily Scrum meetings, Sprint planning. Environment: HDFS, HBase, MapReduce, Hive, Pig, Sqoop, Tableau, NoSQL, Shell Scripting, Oozie, Avro, HDP Distribution, Eclipse, JUnit, Linux. Confidential JAVA/J2EE Developer Responsibilities: Responsible and active in the analysis, design, implementation and deployment of full software development lifecycle(SDLC) of the project. Designed and developed user interface using JSP, HTML and JavaScript. Responsible for developing DAO layer using MVC and manage CRUD operations (insert, update, and delete). Developed and implemented the DAO and service classes. Used Building tools like Maven to build, package, test and deploy application in the application server. Developed Struts action classes, action forms and performed action mapping using Struts framework and performed data validation in form beans and action classes. Extensively used Struts framework as the controller to handle subsequent client requests and invoke the model based upon user requests. Defined the search criteria and pulled out the record of the customer from the database. Make the required changes and save the updated record back to the database. Validated the fields of user registration screen and login screen by writing JavaScript validations. Involved in developing and coding the Interfaces and classes required for the application and created appropriate relationships between the system classes and the interfaces provided. Developed build and deployed scripts using Apache ANT to customize WAR and EAR files. Used DAO and JDBC for database access. Developed stored procedures and triggers using PL/SQL to calculate and update the tables to implement business logic. Used SVN to maintain source and version management. Using JIRA to manage the issues/project work flow. Involved in peer code reviews and performed integration testing of the modules. Followed coding and documentation standards. Involved in post-production support and maintenance of the application. Environment: Oracle, Java, Struts, Servlets, HTML, XML, SQL, J2EE, JUnit, Tomcat, PL/SQL, JIRA, SVN, IntelliJ. Confidential Web Developer Responsibilities: Responsible for creating an effective tool Meta Data Security Manager(MSM), which is known as the heart of Confidential by using HTML, CSS, JavaScript, jQuery AJAX and JSON. Developed many responsive web pages to store client specific information using JavaScript. Performed server side programming using AJAX, JQuery. Configured the hibernate files for the libraries of the project. Implemented bootstrap in designing the responsive design of the web page Developed Use Cases, Class Diagrams and Sequence Diagrams. Used Java Script for client-side Validation. Created unit test cases using Junit for the end-end testing. Effectively participated in every day agile scrum stand-up meetings and participated in Iteration planning. Active participation throughout the entire Software Development Lifecycle(SDLC) from project inception, to code development and elaborate testing of the various modules. Organized inter module knowledge transfer sessions without effecting the productivity of teammates. Involved in configuration and integration of the applications and fixing the defects logged by testing team. Won the “Best Team Award” for the year 2011 as an appreciation from the Confidential Company for being an exceptional team player and meeting challenging job requirements. Received “Employee of the Quarter” award for excelling in the project. Involved in supporting activities for production, pre-prod and other Dev environments. project management experience which includes, but not limited to Preproduction Support, Requirement Analysis, Design Documentation, Setting up Environments. Environment: HTML, JavaScript, AngularJS, AJAX, CSS, JSON, jQuery, Eclipse, UML, Hibernate, MVC, SVN, Putty, Oracle 11g, Sql developer, Netezza and HP Quality Center. 