SUMMARY 1+ years of experience on Big Data Technologies like Hadoop, Hive, Pig, Spark, Kafka, Sqoop, Flume, Hbase Creative and analytical big data developer. Focused on finding new solutions, increasing performance and learning new tools and technology to build a solid big data architecture. Excellent Knowledge in understanding Big Data infrastructure, distributed file systems - HDFS, parallel processing - MapReduce framework and complete Hadoop ecosystem. Experience in processing large sets of structured, semi-structured and unstructured data and supporting systems application architecture. Experienced Hadoop/Java developer and Spark/Scala having end to end experience in developing applications in Hadoop ecosystem. Experience in understanding the clientâ€™s Big Data business requirements and transform it into Hadoop centric technologies. Excellent hands on experience in analyzing data using HiveQL (HQL), Pig Latin, Hbase and MapReduce programs in Java. Efficient in building Hive and Pig scripts. Experience loading data to Hive partitions and creating buckets in Hive. Experienced in importing-exporting RDBMS data into HDFS using Sqoop. Good knowledge of all phases of the Iterative Software Development Life Cycle (SDLC) and Strong independent team player. Experience in loading streaming data into HDFS and analyzing the data with tools like Flume and Hive. Hands on experience in application development using Java, Scala, RDBMS, and Linux/Unix shell scripting. Experience with Oozie Workflow Engine in running workflow jobs with actions that run Hadoop Map/Reduce and Pig jobs. Valuable experience as a Java Developer in Web/intranet, client/server technologies using Java, J2EE, Spring, Servlets, JSP, EJB, JDBC, other web components like HTML, CSS, Java Script. Experience in System study, Requirements Analysis, Design, Development, Testing, Debugging, Deploying, Training, Documentation and Implementation of IT projects using SDLC, Waterfall and Agile Methodology. Major strengths are familiarity with multiple software systems, ability to learn quickly modern technologies, adapt to new environments, self-motivated, team player, focused adaptive and quick learner with excellent interpersonal, technical and communication skills. TECHNICAL SKILLS Operating Systems: Windows, Linux Big Data Technologies: Hadoop, HIVE, Apache Spark, HDFS, PIG, HBase, SQOOP, MapReduce Programming Languages: C++, Java, .Net Scripting Languages: Python, R Database Technologies: SQL, NoSQL, MongoDB, MS-SQL Server, Oracle, MySQL Content Management: WordPress, OpenCart, Magento, Drupal Web Technologies: HTML, Java Script Data Visualization: Tableau, PowerBI Others: Kafka, NIFI, Scala, Oozie PROFESSIONAL EXPERIENCE Big Data Developer Confidential Responsibilities: Built dataflows using NiFi andNiFi Registry to ingest streams of data into data lake Developed Spark programs in Scala that performs ETL to simplify data model and reporting logic for a reporting solution Researched, developed new solutions, and demonstrated feasibility and performance to build, support and optimize the project from early stages Integrated Spark Streaming and Kafka for encryption of streaming data Designed Tableau visualizations for streaming data Maintained Apache Hadoop clusters for application development and Hadoop tools like MapReduce, Hive, Pig, HBase, Flume and Sqoop. Worked with different data format such as JSON, XML, CSV. Created HDFS (Hadoop Distributed File System), and MapReduce jobs in java. Developed MapReduce Programs for data parsing and data cleaning. Used Pig as ETL tool to do transformations, event joins and some pre-aggregations before storing the data onto HDFS. Responsible for developing data pipeline using flume to extract data from weblogs and store in HDFS. Imported and exported Data from/to Different Relational Data Sources like RDBMS to/from HDFS using Sqoop. Created Hive tables and involved in data loading and writing Hive UDFs. Used Hive to analyze the partitioned and bucketed data and compute various metrics for reporting. Automated workflows using shell scripts to pull data from various databases into Hadoop. Cluster coordination services through Zookeeper. Created Oozie workflows to run multiple Hive and Pig scripts. Traversed Hadoop system to identify job failure issues. Designing, Developing and testing reports using Tableau. Software Developer Confidential Responsibilities: Worked with solution architect to review Business Requirements Documentation (BRD) And finalize technical specifications for design and development Created and maintained web pages using HTML, CSS, PHP, JavaScript and jQuery Used AngularJS as framework to create a single page application (SPA) which can bind data to specific views Used jQuery, a cross browser JavaScript Library to dynamically update the page content on the client side Experience in developing and designing web applications pages, reports and components Experienced in creating web sites that are compatible across multiple browsers