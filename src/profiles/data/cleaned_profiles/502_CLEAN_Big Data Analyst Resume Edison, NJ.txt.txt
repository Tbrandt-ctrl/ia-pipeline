SUMMARY 9+ years of overall IT experience in a variety of industries which includes hands on experience of 3 years in Big Data technologies and extensive experience of 4+ years in Java In depth understanding/knowledge of Hadoop Architecture and various components such as HDFS, Job Tracker, Task Tracker, Name Node, Data Node, and MapReduce concepts and experience in working with MapReduce programs using Apache Hadoop for working with Big Data to analyze large data sets efficiently Hands on experience in working with Ecosystems like Hive, Pig, Sqoop, Map Reduce, Flume, Oozie.Strong knowledge of Pig and Hive’s analytical functions, extending Hive and Pig core functionality by writing custom UDFs Experience in importing and exporting terra bytes of data using Sqoop from HDFS to Relational Database Systems and vice - versa Knowledge of job workflow scheduling and monitoring tools like Oozie and Zookeeper, of NoSQL databases such as HBase, Cassandra, and of administrative tasks such as installing Hadoop, Commissioning and decommissioning, and its ecosystem components such as Flume, Oozie, Hive and Pig. Experience in design, development and testing of Distributed, Internet/Intranet/E-Commerce, Client/Server and Database applications mainly using technologies Java, EJB, Servlets, JDBC, JSP, Struts, Hibernate, Spring, JavaScript on WebLogic, Apache Tomcat Web/Application Servers and with Oracle and SQL Server Databases on Unix, windows NT platforms Extensive work experience in Object Oriented Analysis and Design Java/J2EE technologies including HTML, XHTML, DHTML, JavaScript, JSTL, CSS, AJAX and Oracle for developing server-side applications and user interfaces. Experience in developing Middle-tier components in distributed transaction management system using Java. Good understanding of XML methodologies (XML, XSL, XSD) including Web Services and SOAP Extensive experience in working with different databases such as Oracle, IBM DB, RDBMS, SQL Server, MySQL and writing Stored Procedures, Functions, Joins and Triggers for different Data Models. Experience with Agile Methodology, Scrum Methodology, software version control and release management Handled several techno-functional responsibilities including estimates, identifying functional and technical gaps, requirements gathering, designing solutions, development, developing documentation, and production support An individual with excellent interpersonal and communication skills, strong business acumen, creative problem-solving skills, technical competency, team-player spirit, and leadership skills TECHNICAL SKILLS Database: Teradata, DB2, MySQL, Oracle, MS SQL Server, IMS/DB Languages: Java, PIG Latin, SQL, HiveQL, Shell Scripting, and XML API’s/Tools: Mahout, Eclipse, Log4j, Maven Web Technologies: HTML, XML, JavaScript BigData Ecosystem: HDFS, PIG, MAPREDUCE, HIVE, SQOOP, FLUME, OOZIE, HBase, Mongodb, AWS, Solr search, Impala, Cassandra, Storm, Flume, Spark, kafka Operating System: Unix, Linux, Windows XP, IBM Z/OS BI Tools: Tableau, Pentaho PROFESSIONAL EXPERIENCE Confidential, Edison, NJ Big Data Analyst Responsibilities: Involved in different phases of Development life including Analysis, Design, Coding, Unit Testing, Integration Testing, Review and Release as per the business requirements. Worked in the Advanced Operational Analytics and Big Data Analysis team. Designed business layer, database layer, and implemented transaction management into the existing architecture. Worked in Agile environment and participated in daily Stand-ups/Scrum Meetings. Worked on NOSQL databases such as MongoDB, HBase and Cassandra to enhance scalability and performance. Created Load Balancer on AWS EC2 for stable cluster and services which provide fast and effective processing of Data. Connected to Amazon Redshift through Tableau to extract live data for real time analysis. Used AWS Lambda to perform data validation, filtering, sorting or other transformations for every data change in HBase table and load the transformed data to another data store. Integrated Hadoop frameworks/technologies such as Hive and HBase to further operational and analytical experience. Loaded data from different servers to S3 bucket and setting appropriate bucket permissions. Created Hive queries for supporting the existing application. Wrote the HiveQL and manage Hive Meta store server to control different advanced activities. Worked with statistical analysis patterns and create the dashboards for quick references and share to the internal customers on daily, weekly or monthly basis. Worked on partitioning Hive tables and running scripts parallel to reduce run time of the scripts. Implemented business logic by writing UDFs and configuring CRON Jobs. Worked with streaming and Data ware housing projects. Installed and configured Hive and written Hive UDFs. Worked in Json scripts, mongo dB and Unix environment to non-Sql data clean-up grouping and create the analysis reports. Wrote python scripts and java coding for business applications and MapReduce programs. Worked with hive warehouse directory and hive tables and services. Used Spark shell for interactive data analysis and process using Spark Sql to query structured data. Created Stored Procedures to communicate with SQL database. Involved in writing complex SQL Queries and provided SQL Scripts for the Configuration Data which is used by the application. Developed Tableau data visualization using Cross tabs, Heat maps, Box and Whisker charts, Scatter Plots, Geographic Map, Pie Charts and Bar Charts and Density Chart. Used Tableau to generate dashboards and the statistical reports and created a portal using the Tableau JavaScript API Worked closely with business analyst for requirement gathering and translating into technical documentation. Environment: NOSQL, MongoDB 3.6, HBase 1.2, Cassandra, AWS, EC2, Agile, Amazon Redshift, Hadoop frameworks, S3, UDFs, Json, Scripts, UNIX, MapReduce, Python, R, Tableau Confidential, Hoffman Estate, IL Big Data Analyst Responsibilities: Analyzed large data sets by running Hive queries and Pig scripts Complex pig udf for business transformations Worked with the Data Science team, Teradata team, and business to gather requirements for various data sources like webscrapes, APIs Involved in creating Hive/Impala tables, and loading and analyzing data using hive queries Developed Simple to complex MapReduce Jobs using Hive and Pig Involved in running Hadoop jobs for processing millions of records and compression techniques Developed multiple MapReduce jobs in java for data cleaning and pre-processing Involved in loading data from LINUX file system to HDFS, and wrote shell scripts for productionizing the MAP (Member Analytics Platform) project Load and transform large sets of structured, and semi structured data Loaded Golden collection to Apache Solrusing morphline code for Business team Assisted in exporting analyzed data to relational databases using Sqoop Data Modelled for HBase for large transaction sales data Proof of Concept on Strom for streaming the data from one of the sources Proof of Concept in Pentaho for Big Data Implementation of one of the data source transformations in spark Worked in Agile methodology and used Ice Scrum for Development and tracking the project Environment: CDH5.0, Hadoop, HDFS, Pig, Hive, IMPALA, Solr, Morphline, MapReduce, Sqoop, HBase, shell, Pentaho, spark, Teradata, Storm, spark and Big Data Confidential, Sunnyvale, CA Big Data Engineer Responsibilities: Worked on analyzing Hadoop cluster using different big data analytic tools including Hive, and MapReduce Worked on debugging, performance tuning of Hive Jobs Migrating tables from RC format to ORC and data induction and other customized file formats. Wrote Autosys jobs to schedule the reports Implemented test scripts to support test driven development and continuous integration Involved in loading data from LINUX file system to HDFS Supported MapReduce Programs those are running on the cluster Gained experience in managing and reviewing Hadoop log files Involved in scheduling Oozie workflow engine to run multiple Hive jobs Worked on storm and kafka to get the stream of json data Proof of concept on spark for interaction source transformations Used Apache solr to search for specific products each cycle for the business Designing and documenting the project use cases, writing test cases, leading offshore team, and interacting with client. Environment: Hadoop, HDFS, Hive, MapReduce, Oozie, Autosys, shell, Big Data, storm, kafka, flume and spark. Confidential, Melville, NY Hadoop Developer Responsibilities: Analyzed large data sets by running Hive queries and Pig scripts Worked with the Data Science team to gather requirements for various data mining projects Involved in creating Hive tables, and loading and analyzing data using hive queries Developed Simple to complex MapReduce Jobs using Hive and Pig Involved in running Hadoop jobs for processing millions of records of text data Worked with application teams to install operating system, Hadoop updates, patches, version upgrades as required Developed multiple MapReduce jobs in java for data cleaning and preprocessing Involved in loading data from LINUX file system to HDFS Responsible for managing data from multiple sources Responsible to manage data coming from various sources Assisted in exporting analyzed data to relational databases (MySQL) using Sqoop Created and maintained Technical documentation for launching HADOOP Clusters and for executing Hive queries and Pig Scripts Generating tableau reports and building dashboards Worked closely with business units to define development estimates according to Agile Methodology CDH 4.6: 48 nodes having each node of 3TB storage and 32GB ram Environment: Hadoop, HDFS, Pig, Hive, MapReduce, Cassandra, LINUX, Tableau 8.2, shell scripting, and Big Data Confidential, East Hartford, CT Hadoop Developer Responsibilities: Worked on analyzing Hadoop cluster using different big data analytic tools including Pig, Hive, and MapReduce on EC2 Collecting and aggregating copious amounts of log data using Apache Flume and staging data in HDFS for further analysis Worked on debugging, performance tuning of Hive & Pig Jobs Created HBase tables to store various data formats of PII data coming from different portfolios Implemented test scripts to support test driven development and continuous integration Worked on tuning the performance Pig queries Involved in loading data from LINUX file system to HDFS Importing and exporting data into HDFS and HBase using Sqoop from MYSQL Experience working on processing semi-structured data using Pig and Hive Supported MapReduce Programs those are running on the cluster Gained experience in managing and reviewing Hadoop log and JSON files Involved in scheduling Oozie workflow engine to run multiple Hive and pig jobs Created and maintained Technical documentation for launching HADOOP Clusters and for executing Hive queries and Pig Scripts Environment: Hadoop, HDFS, HBase, Pig, Hive, MapReduce, Sqoop, Oozie, LINUX, S3, EC2, AWS and Big Data Confidential, St. Louis, MO  Sr. Java Developer Responsibilities: Created design documents and reviewed with team in addition to assisting the business analyst / project manager in explanations to line of business. Worked as part of the Agile Application Architecture (A3) development team responsible for setting up the architectural components for different layers of the application. Responsible for understanding the scope of the project and requirement gathering. Involved in analysis, design, construction and testing of the application Developed the web tier using JSP to show account details and summary Used web services Client for making calls to data. Generated Client classes using WSDL2Java and used the generated Java API Designed and developed the UI using JSP, HTML, CSS and JavaScript Utilized JPA for Object/Relational Mapping purposes for transparent persistence onto the SQL Server database. Used Tomcat web server for development purpose. Involved in creation of Test Cases for JUnit Testing. Used Oracle as Database and used Toad for queries execution and also involved in writing SQL scripts, PL/SQL code for procedures and functions. Used CVS for version controlling. Developed application using Eclipse and used build and deploy tool as Maven. Used Log4J to print the logging, debugging, warning, info on the server console. Environment: Java, J2EE Servlet, JSP, JUnit, AJAX, XML, JavaScript, Log4j, CVS, Maven, Eclipse, Apache Tomcat, and Oracle. Confidential Sr. Java Developer Responsibilities: Developed web components using JSP, Servlets and JDBC Designed tables and indexes Designed, Implemented, Tested and Deployed Enterprise Java Beans both Session and Entity using WebLogic as Application Server Developed stored procedures, packages and database triggers to enforce data integrity. Performed data analysis and created crystal reports for user requirements Provided quick turn around and resolving issues within the SLA. Implemented the presentation layer with HTML, XHTML and JavaScript Used EJBs to develop business logic and coded reusable components in Java Beans Development of database interaction code to JDBC API making extensive use of SQL Query Statements and advanced Prepared Statements. Used connection pooling for best optimization using JDBC interface Used EJB entity and session beans to implement business logic and session handling and transactions. Developed user-interface using JSP, Servlets, and JavaScript Wrote complex SQL queries and stored procedures Actively involved in the system testing Prepared the Installation, Customer guide and Configuration document which were delivered to the customer along with the product Involved in development, and Testing, phases of the project by following Agile methodology Environment: Windows NT 2000/2003, XP, and Windows 7/ 8 C, Java, UNIX, and SQL using TOAD, Finacle Core banking, CRM 10209, Microsoft Office Suit, Microsoft project Confidential Java Developer Responsibilities: Involved Confidential requirement gathering & Analysis of the project Designed the functional specifications and architecture of the web-based module using Java Technologies. Created Design specification using UML Class Diagrams, Sequence &Activity Diagrams Developed the Web Application using MVC Architecture, Java, JSP, and Servlets & Oracle Database. Developed various Java classes, SQL queries and procedures to retrieve and manipulate the data from backend Oracle database using JDBC Extensively worked with Java Script for front-end validations. Analysis of business requirements and develop system architecture document for the enhancement project. Designed and developed applications on Service Oriented Architecture (SOA) Created UML (Use cases, Class diagrams, Activity diagrams, Component diagrams, etc.) using Visio Provided Impact Analysis and Test cases. Delivered the code within the timeline, and logged the bugs/fixes in TechOnline, tracking system I had developed Unit & Functional Test cases for testing Web Application Adopted Agile Development along with methodologies such as pair programming and Test-driven development. Environment: Windows NT 2000/2003, XP, and Windows 7/ 8, C, Java, UNIX, and SQL 